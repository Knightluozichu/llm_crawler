import base64
from io import BytesIO
from pathlib import Path
from typing import Dict

import pandas as pd
import streamlit as st

from data_processor import DataProcessor
from visualizer import DataVisualizer

# ========== LLM ç›¸å…³ç¤ºä¾‹å‡½æ•° ==========

import requests
import time
import jieba
from requests.exceptions import ConnectionError

OLLAMA_BASE_URL = "http://127.0.0.1:11434"


def get_local_models():
    """
    è·å– Ollama æœ¬åœ°å¯ç”¨æ¨¡å‹åˆ—è¡¨(ç¤ºä¾‹)ã€‚
    è‹¥è¿æ¥å¤±è´¥æˆ–æ¥å£ç»“æ„æœ‰å˜ï¼Œå¯è‡ªè¡Œè°ƒæ•´ã€‚
    """
    max_retries = 3
    retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                return [model['name'] for model in models]
            else:
                print(f"APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                return ["æ— æ³•è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨"]
        except ConnectionError:
            if attempt < max_retries - 1:
                print(f"è¿æ¥OllamaæœåŠ¡å¤±è´¥, {retry_delay}ç§’åé‡è¯•...")
                time.sleep(retry_delay)
            else:
                print("æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡, è¯·ç¡®ä¿æœåŠ¡å·²å¯åŠ¨")
                return ["æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡"]
        except Exception as e:
            print(f"è·å–æ¨¡å‹åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return [f"è·å–æœ¬åœ°æ¨¡å‹åˆ—è¡¨æ—¶å‡ºé”™: {str(e)}"]


def parse_resume(file_bytes, file_type):
    """
    å ä½å‡½æ•°ï¼šè§£æç”¨æˆ·ä¸Šä¼ çš„ç®€å†ï¼ˆPDFæˆ–Wordï¼‰ï¼Œè¿”å›æ–‡æœ¬å†…å®¹ã€‚
    å¯ä½¿ç”¨ PyMuPDFã€PyPDF2ã€python-docx ç­‰åº“è¿›è¡ŒçœŸå®è§£æã€‚
    """
    try:
        if file_type == "pdf":
            return "ã€PDFç®€å†ç¤ºä¾‹ã€‘è¿™é‡Œæ˜¯è§£æåçš„ç®€å†æ–‡æœ¬..."
        elif file_type in ["docx", "doc"]:
            return "ã€Wordç®€å†ç¤ºä¾‹ã€‘è¿™é‡Œæ˜¯è§£æåçš„ç®€å†æ–‡æœ¬..."
        else:
            return "æš‚ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œæ— æ³•è§£æã€‚"
    except Exception:
        return "è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç®€å†æ–‡ä»¶æ ¼å¼æˆ–å†…å®¹ã€‚"


def match_jobs_with_resume(resume_text, job_df, llm_mode, openai_key=None):
    """
    å°†ç®€å†ä¸ job_df è¿›è¡Œç®€å•åŒ¹é…ï¼Œè¿”å›å‰ 10 æ¡æœ€åŒ¹é…ç»“æœ (æ¼”ç¤º)ã€‚
    å®é™…å¯æ”¹ä¸ºå¤æ‚ç®—æ³•æˆ–è°ƒç”¨å¤§æ¨¡å‹ã€‚
    
    :param resume_text: è§£æåçš„ç®€å†æ–‡æœ¬
    :param job_df: å·²ç­›é€‰/å¤„ç†åçš„ DataFrame
    :param llm_mode: "local" æˆ– "openai"
    :param openai_key: è‹¥ä½¿ç”¨ openaiï¼Œéœ€è¦æä¾› key
    :return: list of dict
    """
    if not resume_text or job_df.empty:
        return []

    # åˆ†è¯ç®€å†æ–‡æœ¬
    resume_tokens = set(jieba.lcut(resume_text.lower()))
    
    results = []
    for idx, row in job_df.iterrows():
        summary = str(row.get('job_summary', '')).lower()
        salary = str(row.get('salary', 'é¢è®®'))
        job_name = str(row.get('position_name', 'æœªçŸ¥å²—ä½'))
        comp_name = str(row.get('company_name', 'æœªçŸ¥å…¬å¸'))
        
        # åˆ†è¯å²—ä½æè¿°
        summary_tokens = set(jieba.lcut(summary))
        
        # ç®€æ˜“åŒ¹é…åº¦ = äº¤é›†è¯æ•° / (å²—ä½å…³é”®è¯æ€»æ•° + 1)
        common_tokens = resume_tokens.intersection(summary_tokens)
        match_score = len(common_tokens) / (len(summary_tokens) + 1)
        
        match_reason = f"ç®€å†ä¸å²—ä½æè¿°å­˜åœ¨ {len(common_tokens)} ä¸ªç›¸åŒå…³é”®è¯"
        
        results.append({
            "job_name": job_name,
            "company_name": comp_name,
            "match_score": round(match_score, 2),
            "match_reason": match_reason,
            "salary_range": salary,
            "job_index": idx
        })
    
    # æ’åºå–å‰ 10
    results.sort(key=lambda x: x["match_score"], reverse=True)
    return results[:10]


def modify_resume_for_job(original_resume, job_description, llm_mode, openai_key=None):
    """
    åŸºäº original_resume ä¸ job_descriptionï¼Œç”Ÿæˆä¸ªæ€§åŒ–ä¿®æ”¹åçš„ç®€å†æ–‡æœ¬ (æ¼”ç¤º)ã€‚
    """
    new_resume = (
        "ã€AIä¿®æ”¹åçš„ç®€å†ç¤ºä¾‹ã€‘\n\n"
        f"=== åŸç®€å†éƒ¨åˆ†å†…å®¹ ===\n{original_resume[:100]}...\n\n"
        f"=== ç›®æ ‡å²—ä½éœ€æ±‚ ===\n{job_description}\n\n"
        "=== ä¼˜åŒ–åç®€å†ç¤ºä¾‹ ===\n"
        "æ ¹æ®å²—ä½éœ€æ±‚åŒ¹é…æŠ€èƒ½äº®ç‚¹ï¼Œçªå‡ºç›¸å…³é¡¹ç›®ç»éªŒåŠæŠ€æœ¯æ ˆã€‚"
    )
    return new_resume


# ========== ä¸» UI ç±» ==========

class JobUI:
    """
    Streamlit UI ä¸»ç•Œé¢å°è£…ç±»ã€‚
    - è´Ÿè´£åŠ è½½æ•°æ®ï¼Œç®¡ç†ä¼šè¯çŠ¶æ€ï¼Œå¹¶ä¸ DataProcessorã€DataVisualizer é…åˆè¿›è¡Œå¯è§†åŒ–ä¸æ±‚èŒåŠŸèƒ½ã€‚
    """
    def __init__(self, data_root: Path):
        self.data_root = data_root
        self.data_dir = data_root / 'data'
        self._init_session_state()
        self.data_processor = None
        self.visualizer = None
        
    def _init_session_state(self):
        """åˆå§‹åŒ–ä¼šè¯çŠ¶æ€"""
        if 'resume_text' not in st.session_state:
            st.session_state['resume_text'] = ""
        if 'llm_mode' not in st.session_state:
            st.session_state['llm_mode'] = "æœ¬åœ° Ollama"
        if 'local_model' not in st.session_state:
            st.session_state['local_model'] = ""
        if 'openai_key' not in st.session_state:
            st.session_state['openai_key'] = ""
        if 'data_loaded' not in st.session_state:
            st.session_state['data_loaded'] = False

    def _load_data(self):
        """
        ä¾§è¾¹æ æˆ–é¡µé¢é€‰æ‹© CSV æ–‡ä»¶å¹¶åŠ è½½ï¼Œ
        ç”¨ DataProcessor è¿›è¡Œé¢„å¤„ç†ï¼Œç„¶ååˆå§‹åŒ– DataVisualizerã€‚
        """
        csv_files = [f for f in self.data_dir.glob('*.csv')]
        if not csv_files:
            st.error("æ•°æ®ç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶ï¼Œè¯·å…ˆå‡†å¤‡å¥½æ•°æ®æ–‡ä»¶ã€‚")
            return None
        
        selected_file = st.selectbox("è¯·é€‰æ‹©æ•°æ®æ–‡ä»¶", [f.name for f in csv_files])
        
        if st.button("åŠ è½½å¹¶åˆ†ææ•°æ®") or st.session_state.get('data_loaded', False):
            data_file = self.data_dir / selected_file
            try:
                if not data_file.exists():
                    st.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
                    return None
                    
                # åªæœ‰åœ¨æœªåŠ è½½æˆ–æ–‡ä»¶å˜åŒ–æ—¶æ‰é‡æ–°åŠ è½½
                if (not self.data_processor or 
                    str(data_file) != getattr(self.data_processor, 'current_file', None)):
                    self.data_processor = DataProcessor(data_file)
                    self.visualizer = DataVisualizer(self.data_processor)
                    # ä¿å­˜å½“å‰æ–‡ä»¶è·¯å¾„ä»¥ä¾¿åç»­æ¯”è¾ƒ
                    self.data_processor.current_file = str(data_file)
                
                st.session_state['data_loaded'] = True
                st.success(f"æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {selected_file}")
                
                return True
                
            except Exception as e:
                st.error(f"åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
                return None
        
    def _setup_llm_settings(self):
        """
        ä¾§è¾¹æ ï¼šè®¾ç½® LLM ç›¸å…³é…ç½®
        """
        st.sidebar.header("AI è®¾ç½®")
        llm_mode = st.sidebar.radio(
            "é€‰æ‹© LLM æ¨¡å¼",
            options=["æœ¬åœ° Ollama", "OpenAI åœ¨çº¿æ¨¡å‹"],
            index=0 if st.session_state['llm_mode'] == "æœ¬åœ° Ollama" else 1
        )
        st.session_state['llm_mode'] = llm_mode

        if llm_mode == "æœ¬åœ° Ollama":
            local_models = get_local_models()
            selected_model = st.sidebar.selectbox(
                "é€‰æ‹©æœ¬åœ°æ¨¡å‹",
                local_models,
                key="local_model_select"
            )
            st.session_state['local_model'] = selected_model
        else:
            openai_key = st.sidebar.text_input(
                "è¾“å…¥ OpenAI API Key (å¿…å¡«)",
                value=st.session_state.get('openai_key', ""),
                type="password",
                key="openai_key_input"
            )
            st.session_state['openai_key'] = openai_key

    def setup_sidebar(self) -> Dict:
        """
        ä¾§è¾¹æ ï¼šè®¾ç½®å¤šé¡¹ç­›é€‰æ¡ä»¶å¹¶è¿”å›ç­›é€‰å™¨å­—å…¸ã€‚
        - è–ªèµ„èŒƒå›´ (min, max) (å•ä½ï¼šåƒå…ƒ)
        - work_exp (æœ€å¤§å¹´é™)
        - education (0~4)
        - company_type (å¤šé€‰)
        - welfare_tags (å¤šé€‰)
        
        :return: dict æ ¼å¼çš„å„é¡¹ç­›é€‰æ¡ä»¶
        """
        st.sidebar.header("ç­›é€‰æ¡ä»¶")
        
        if not (self.data_processor and self.visualizer):
            return {}
        
        df = self.visualizer.processed_data
        
        # è–ªèµ„èŒƒå›´
        min_val = int(df['avg_salary'].min()) if len(df) else 0
        max_val = int(df['avg_salary'].max()) if len(df) else 50
        salary_range = st.sidebar.slider(
            "è–ªèµ„èŒƒå›´ (åƒå…ƒ)",
            min_value=min_val,
            max_value=max_val,
            value=(min_val, max_val)
        )
        
        # å·¥ä½œç»éªŒ (work_exp)
        max_exp = int(df['work_exp'].max()) if len(df) else 10
        selected_exp = st.sidebar.slider(
            "æœ€å¤§å·¥ä½œç»éªŒ (å¹´)",
            min_value=0,
            max_value=max_exp,
            value=max_exp
        )
        
        # å­¦å†è¦æ±‚
        edu_options = ['ä¸é™', 'å¤§ä¸“', 'æœ¬ç§‘', 'ç¡•å£«', 'åšå£«']
        education = st.sidebar.select_slider(
            "æœ€ä½å­¦å†è¦æ±‚",
            options=edu_options,
            value='ä¸é™'
        )
        education_idx = edu_options.index(education)
        
        # å…¬å¸ç±»å‹
        company_types = df['company_type'].unique().tolist()
        selected_company_types = st.sidebar.multiselect(
            "å…¬å¸ç±»å‹",
            options=company_types,
            default=company_types
        )
        
        # ç¦åˆ©æ ‡ç­¾
        all_welfare_tags = []
        for tags in df['welfare_tags']:
            all_welfare_tags.extend(tags)
        welfare_options = list(set(all_welfare_tags))
        selected_welfare_tags = st.sidebar.multiselect(
            "ç¦åˆ©æ ‡ç­¾",
            options=welfare_options,
            default=[]
        )
        
        return {
            'salary_range': salary_range,
            'work_exp': selected_exp,
            'education': education_idx,
            'company_type': selected_company_types,
            'welfare_tags': selected_welfare_tags
        }

    def _handle_resume_upload(self):
        """
        é¡µé¢ä¸­ï¼šä¸Šä¼ å¹¶è§£æç®€å†æ–‡ä»¶ï¼ˆPDFã€Wordï¼‰ã€‚
        è§£æåå­˜å…¥ session_state['resume_text']ã€‚
        """
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ ç®€å†ï¼ˆPDFæˆ–Wordï¼‰",
            type=["pdf", "doc", "docx"],
            key="resume_uploader"
        )
        
        if uploaded_file is not None:
            file_type = uploaded_file.name.split('.')[-1].lower()
            file_bytes = uploaded_file.read()
            
            new_text = parse_resume(file_bytes, file_type)
            if new_text != st.session_state['resume_text']:
                st.session_state['resume_text'] = new_text
                st.success("ç®€å†ä¸Šä¼ å¹¶è§£ææˆåŠŸï¼")
                st.write("ç®€å†è§£æç»“æœï¼š")
                st.write(new_text[:300] + "..." if len(new_text) > 300 else new_text)

    def _show_basic_analysis_tab(self, tab):
        """
        é€‰é¡¹å¡ 1ï¼šæ˜¾ç¤ºåŸºç¡€å¯è§†åŒ–åˆ†æï¼ŒåŒ…æ‹¬è–ªèµ„ã€å­¦å†ã€ç»éªŒã€å…¬å¸ç±»å‹åˆ†å¸ƒç­‰ã€‚
        """
        with tab:
            st.subheader("åŸºç¡€åˆ†æ")
            if not self.visualizer:
                st.warning("è¯·å…ˆåŠ è½½æ•°æ®")
                return
            
            st.plotly_chart(self.visualizer.plot_salary_distribution(), use_container_width=True)
            
            col1, col2 = st.columns(2)
            with col1:
                st.plotly_chart(self.visualizer.plot_education_pie(), use_container_width=True)
            with col2:
                st.plotly_chart(self.visualizer.plot_experience_bar(), use_container_width=True)
                
            col3, col4 = st.columns(2)
            with col3:
                st.plotly_chart(self.visualizer.plot_company_type_pie(), use_container_width=True)
            with col4:
                st.info("æ›´å¤šå›¾è¡¨å¯åœ¨æ­¤æ‰©å±•ï¼Œå¦‚è¡Œä¸šåˆ†å¸ƒç­‰...")
            
            wordcloud_data = self.visualizer.generate_job_wordcloud()
            if wordcloud_data:
                st.image(
                    BytesIO(base64.b64decode(wordcloud_data)), 
                    caption='èŒä½æè¿°å…³é”®è¯äº‘å›¾'
                )
            else:
                st.warning("æ— æ³•ç”Ÿæˆè¯äº‘ï¼Œå¯èƒ½æ— æœ‰æ•ˆèŒä½æè¿°æ•°æ®")

    def _show_insights_tab(self, tab):
        """
        é€‰é¡¹å¡ 2ï¼šæ˜¾ç¤ºå²—ä½æ´å¯ŸæŠ¥è¡¨ï¼ŒåŒ…æ‹¬è–ªèµ„ã€å…³é”®è¯ç­‰å…³é”®æŒ‡æ ‡ã€‚
        """
        with tab:
            st.subheader("å²—ä½æ´å¯ŸæŠ¥è¡¨")
            if not self.visualizer:
                st.warning("è¯·å…ˆåŠ è½½æ•°æ®")
                return
            
            insights = self.visualizer.generate_job_insights()
            
            # å±•ç¤ºå…³é”®è–ªèµ„æŒ‡æ ‡
            col1, col2, col3 = st.columns(3)
            col1.metric("å¹³å‡è–ªèµ„", f"{insights['salary']['avg']} åƒå…ƒ")
            col2.metric("æœ€ä½è–ªèµ„", f"{insights['salary']['min']} åƒå…ƒ")
            col3.metric("æœ€é«˜è–ªèµ„", f"{insights['salary']['max']} åƒå…ƒ")
            
            # å±•ç¤ºé«˜é¢‘å…³é”®è¯
            st.write("èŒä½æè¿°é«˜é¢‘å…³é”®è¯ TOP 10ï¼š", ", ".join(insights['keywords']))
            
            # ç»˜åˆ¶æ´å¯Ÿå¯è§†åŒ–
            fig_salary, fig_keywords = self.visualizer.plot_insights_summary(insights)
            st.plotly_chart(fig_salary, use_container_width=True)
            st.plotly_chart(fig_keywords, use_container_width=True)
            
            # å¯¼å‡ºæ´å¯Ÿ
            if st.button("å¯¼å‡ºæ´å¯ŸæŠ¥è¡¨"):
                df_insights = pd.DataFrame.from_dict(insights, orient='index')
                csv_str = df_insights.to_csv()
                st.download_button(
                    label="ä¸‹è½½ CSV æŠ¥è¡¨",
                    data=csv_str,
                    file_name='job_insights.csv',
                    mime='text/csv'
                )

    def _show_job_search_tab(self, tab):
        """
        é€‰é¡¹å¡ 3ï¼šæ±‚èŒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ç®€å†ä¸Šä¼ ã€åŒ¹é…ä¸ç®€å†å®šåˆ¶åŒ–ä¿®æ”¹ã€‚
        """
        with tab:
            st.subheader("æ±‚èŒä¸­å¿ƒï¼šç®€å†åŒ¹é…ä¸å®šåˆ¶åŒ–ä¿®æ”¹")
            
            # ä¸Šä¼ ç®€å†
            self._handle_resume_upload()
            
            # è‹¥å·²ä¸Šä¼ ç®€å†ï¼Œæ˜¾ç¤ºå½“å‰ç®€å†æ¦‚è§ˆ
            if st.session_state['resume_text']:
                st.write("---")
                st.subheader("å·²è§£æçš„ç®€å†å†…å®¹")
                display_text = (
                    st.session_state['resume_text'][:300] + "..." 
                    if len(st.session_state['resume_text']) > 300 
                    else st.session_state['resume_text']
                )
                st.write(display_text)
            
            st.warning("è¯·æ³¨æ„ä¿æŠ¤ä¸ªäººä¿¡æ¯éšç§ï¼Œå¦‚ä½¿ç”¨åœ¨çº¿æ¨¡å‹æ—¶éœ€ç¡®ä¿å·²äº†è§£ç›¸å…³é£é™©ã€‚")
            
            # å¼€å§‹åŒ¹é…
            if st.button("å¼€å§‹åŒ¹é…"):
                if not st.session_state['resume_text']:
                    st.error("è¯·å…ˆä¸Šä¼ å¹¶è§£æç®€å†æ–‡ä»¶ï¼")
                else:
                    # å¦‚æœç”¨æˆ·é€‰æ‹©äº†OpenAIæ¨¡å¼ï¼Œä½†æ²¡å¡«key
                    if (st.session_state['llm_mode'] == "OpenAI åœ¨çº¿æ¨¡å‹" and 
                        not st.session_state['openai_key']):
                        st.error("è¯·è¾“å…¥ OpenAI API Keyï¼")
                        return
                    
                    st.info("æ­£åœ¨è¿›è¡Œå²—ä½åŒ¹é…ï¼Œè¯·ç¨å€™...")
                    
                    # ä½¿ç”¨ processed_data è¿›è¡ŒåŒ¹é…
                    df = self.visualizer.processed_data
                    job_matches = match_jobs_with_resume(
                        resume_text=st.session_state['resume_text'],
                        job_df=df,
                        llm_mode=(
                            "local" if st.session_state['llm_mode'] == "æœ¬åœ° Ollama"
                            else "openai"
                        ),
                        openai_key=st.session_state.get('openai_key', "")
                    )
                    
                    if job_matches:
                        st.success("å·²æ‰¾åˆ°å‰ 10 æ¡æœ€åŒ¹é…å²—ä½ï¼š")
                        for i, match in enumerate(job_matches):
                            st.markdown(f"**{i+1}. å²—ä½åç§°:** {match['job_name']}")
                            st.markdown(f"**å…¬å¸åç§°:** {match['company_name']}")
                            st.markdown(f"**åŒ¹é…åº¦è¯„åˆ†:** {match['match_score']}")
                            st.markdown(f"**åŒ¹é…åŸå› :** {match['match_reason']}")
                            st.markdown(f"**è–ªèµ„èŒƒå›´:** {match['salary_range']}")
                            
                            # ç®€å†ä¿®æ”¹åŠŸèƒ½
                            with st.expander(f"ğŸ”§ ä¿®æ”¹ç®€å†ä»¥åŒ¹é…å²—ä½: {match['job_name']}"):
                                if st.button(f"ç”Ÿæˆé’ˆå¯¹ {match['job_name']} çš„ä¼˜åŒ–ç®€å†", key=f"modify_{i}"):
                                    final_resume = modify_resume_for_job(
                                        original_resume=st.session_state['resume_text'],
                                        job_description=(
                                            f"{match['job_name']} | {match['company_name']} | "
                                            f"è–ªèµ„: {match['salary_range']}"
                                        ),
                                        llm_mode=(
                                            "local" if st.session_state['llm_mode'] == "æœ¬åœ° Ollama"
                                            else "openai"
                                        ),
                                        openai_key=st.session_state.get('openai_key', "")
                                    )
                                    st.write("---")
                                    st.subheader("AIä¼˜åŒ–åçš„ç®€å†å†…å®¹:")
                                    st.write(final_resume)
                                    
                                    dl_data = final_resume.encode("utf-8")
                                    st.download_button(
                                        label="ä¸‹è½½ä¿®æ”¹åç®€å† (txt)",
                                        data=dl_data,
                                        file_name="modified_resume.txt",
                                        mime="text/plain"
                                    )
                    else:
                        st.warning("æœªæ‰¾åˆ°åŒ¹é…å²—ä½æˆ–ç®€å†å†…å®¹ä¸è¶³ã€‚")

    def run(self):
        """
        Streamlit åº”ç”¨çš„ä¸»å…¥å£å‡½æ•°
        """
        st.set_page_config(
            page_title="AIå²—ä½åˆ†æå¯è§†åŒ– & æ±‚èŒç³»ç»Ÿ",
            layout="wide",
            initial_sidebar_state="expanded"
        )

        # æ˜¾ç¤ºä¸»ç•Œé¢æ ‡é¢˜
        st.title("AI å²—ä½åˆ†æå¯è§†åŒ– & æ±‚èŒç³»ç»Ÿ")
        
        # ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        load_result = self._load_data()
        
        # åªæœ‰æˆåŠŸåŠ è½½æ•°æ®åæ‰æ˜¾ç¤ºåç»­å†…å®¹
        if load_result or st.session_state.get('data_loaded', False):
            # ç¬¬äºŒæ­¥ï¼šè®¾ç½® LLM é…ç½®ï¼ˆä¾§è¾¹æ ï¼‰
            self._setup_llm_settings()
            
            # ç¬¬ä¸‰æ­¥ï¼šè®¾ç½®ç­›é€‰æ¡ä»¶ï¼ˆä¾§è¾¹æ ï¼‰
            filters = self.setup_sidebar()
            
            # åº”ç”¨ç­›é€‰æ¡ä»¶
            if self.data_processor and self.visualizer and filters:
                filtered_df = self.data_processor.filter_data(filters)
                self.visualizer.processed_data = filtered_df
            
            # ç¬¬å››æ­¥ï¼šåˆ›å»ºå¤šé€‰é¡¹å¡
            tab1, tab2, tab3 = st.tabs(["åŸºç¡€åˆ†æ", "å²—ä½æ´å¯ŸæŠ¥è¡¨", "æ±‚èŒ"])
            
            self._show_basic_analysis_tab(tab1)
            self._show_insights_tab(tab2)
            self._show_job_search_tab(tab3)
        else:
            st.info("è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½æ•°æ®æ–‡ä»¶")
